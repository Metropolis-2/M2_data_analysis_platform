{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "CONF_LOG_PREFIX = 'CONFLOG'\n",
    "FLST_LOG_PREFIX = 'FLSTLOG'\n",
    "GEO_LOG_PREFIX = 'GEOLOG'\n",
    "LOS_LOG_PREFIX = 'LOSLOG'\n",
    "REG_LOG_PREFIX = 'REGLOG'\n",
    "LOADING_PATH = '/mnt/shared/repos/metropolis/M2_data_analysis_platform/output'\n",
    "DATAFRAMES_NAMES = [CONF_LOG_PREFIX, FLST_LOG_PREFIX, GEO_LOG_PREFIX, LOS_LOG_PREFIX, REG_LOG_PREFIX]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Give access to the constants that defines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "sys.path.append(str(Path(Path().absolute().parent, 'platform_code')))\n",
    "from schemas.tables_attributes import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def load_dataframes(files_names: List[str], loading_path: str, spark: SparkSession) -> Dict[str, DataFrame]:\n",
    "    \"\"\" Loads the dataframes which macht the file names passed by arguments.\n",
    "    The method read from the config the path were to read the files, which\n",
    "    matches the folder where the files are saved in `save_dataframes_dict()`.\n",
    "\n",
    "    :param files_names: list of the names of the files.\n",
    "    :param loading_path: path were the files are saved.\n",
    "    :param spark: spark session.\n",
    "    :return: dictionary with the dataframes loaded from the files, with the\n",
    "     file name as key.\n",
    "    \"\"\"\n",
    "    dataframes = dict()\n",
    "\n",
    "    for file_name in files_names:\n",
    "        file_path = Path(loading_path, f'{file_name.lower()}.parquet')\n",
    "        logger.info('Loading dataframe from `{}`.', file_path)\n",
    "        df = spark.read.parquet(str(file_path))\n",
    "        dataframes[file_name] = df\n",
    "\n",
    "    return dataframes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 16:37:08 WARN Utils: Your hostname, GRILUX-DEV resolves to a loopback address: 127.0.1.1; using 192.168.2.150 instead (on interface wlp3s0)\n",
      "22/03/24 16:37:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/24 16:37:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Notebook').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-24 16:37:23.872 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `/mnt/shared/repos/metropolis/M2_data_analysis_platform/output/conflog.parquet`.\n",
      "2022-03-24 16:37:29.531 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `/mnt/shared/repos/metropolis/M2_data_analysis_platform/output/flstlog.parquet`.\n",
      "2022-03-24 16:37:29.782 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `/mnt/shared/repos/metropolis/M2_data_analysis_platform/output/geolog.parquet`.\n",
      "2022-03-24 16:37:30.024 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `/mnt/shared/repos/metropolis/M2_data_analysis_platform/output/loslog.parquet`.\n",
      "2022-03-24 16:37:30.241 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `/mnt/shared/repos/metropolis/M2_data_analysis_platform/output/reglog.parquet`.\n"
     ]
    }
   ],
   "source": [
    "input_dataframes = load_dataframes(DATAFRAMES_NAMES, LOADING_PATH, spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this metrics we are going to work with the combined FLST log and the flight intentions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CAP-1: Average demand delay\n",
    "Although a flight intention delay represents direct measure of the system efficiency, it can also be used as a proxy for the lack of the system capacity as previously explained. The more flights are delayed, the bigger the capacity problem is. Hence, it can be used for the relative comparison of the concepts\n",
    "\n",
    "Average demand delay is computed as the arithmetic mean of the delays of all flight intentions in an scenario."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataframe = input_dataframes[FLST_LOG_PREFIX]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "result = dataframe\\\n",
    "    .select(SCENARIO_NAME, BASELINE_ARRIVAL_TIME, DEL_TIME)\\\n",
    "    .groupby(SCENARIO_NAME)\\\n",
    "    .agg(mean(col(DEL_TIME) - col(BASELINE_ARRIVAL_TIME)).alias(CAP1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|          Scenario|              CAP1|\n",
      "+------------------+------------------+\n",
      "|1_very_low_40_8_R2|174.88071918752135|\n",
      "|3_very_low_40_8_R2|174.88071918752135|\n",
      "|1_very_low_40_8_W1|174.88071918752135|\n",
      "|3_very_low_40_8_W1|174.88071918752135|\n",
      "|2_very_low_40_8_W1|174.88071918752135|\n",
      "|2_very_low_40_8_R2|174.88071918752135|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CAP-2: Average number of intrusions\n",
    "Is a ratio of the total number of intrusions with respect of the number of flight intention in the scenario."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "los_log_df = input_dataframes[LOS_LOG_PREFIX]\n",
    "flst_log_df = input_dataframes[FLST_LOG_PREFIX]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "saf_2_result = los_log_df\\\n",
    "    .groupBy(SCENARIO_NAME)\\\n",
    "    .count()\\\n",
    "    .select(SCENARIO_NAME, col('count').alias(SAF2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|          Scenario|SAF2|\n",
      "+------------------+----+\n",
      "|1_very_low_40_8_W1| 785|\n",
      "|2_very_low_40_8_W1| 785|\n",
      "|1_very_low_40_8_R2| 785|\n",
      "|3_very_low_40_8_W1| 785|\n",
      "|3_very_low_40_8_R2| 785|\n",
      "|2_very_low_40_8_R2| 785|\n",
      "+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saf_2_result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "number_of_flights = flst_log_df\\\n",
    "    .groupby(SCENARIO_NAME)\\\n",
    "    .count()\\\n",
    "    .select([SCENARIO_NAME, col('count').alias('num_flights')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|          Scenario|num_flights|\n",
      "+------------------+-----------+\n",
      "|1_very_low_40_8_R2|       3760|\n",
      "|3_very_low_40_8_R2|       3760|\n",
      "|1_very_low_40_8_W1|       3760|\n",
      "|3_very_low_40_8_W1|       3760|\n",
      "|2_very_low_40_8_W1|       3760|\n",
      "|2_very_low_40_8_R2|       3760|\n",
      "+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "number_of_flights.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "result = saf_2_result.join(number_of_flights, on=SCENARIO_NAME)\\\n",
    "    .withColumn(CAP2, col(SAF2) / col('num_flights'))\\\n",
    "    .select(SCENARIO_NAME, CAP2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|          Scenario|               CAP2|\n",
      "+------------------+-------------------+\n",
      "|1_very_low_40_8_R2|0.20877659574468085|\n",
      "|3_very_low_40_8_R2|0.20877659574468085|\n",
      "|1_very_low_40_8_W1|0.20877659574468085|\n",
      "|3_very_low_40_8_W1|0.20877659574468085|\n",
      "|2_very_low_40_8_W1|0.20877659574468085|\n",
      "|2_very_low_40_8_R2|0.20877659574468085|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CAP-3: Additional demand delay\n",
    "Calculates the magnitude of delay increase (CAP-1) due to the fact of the existence of rogue aircraft."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CAP-4: Additional number of intrusions\n",
    "Calculates the degradation produced in the intrusion safety indicator when rogue aircraft are introduced."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}