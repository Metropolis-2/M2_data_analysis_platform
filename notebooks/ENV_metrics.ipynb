{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CONF_LOG_PREFIX = 'CONFLOG'\n",
    "FLST_LOG_PREFIX = 'FLSTLOG'\n",
    "GEO_LOG_PREFIX = 'GEOLOG'\n",
    "LOS_LOG_PREFIX = 'LOSLOG'\n",
    "REG_LOG_PREFIX = 'REGLOG'\n",
    "LOADING_PATH = '../output'\n",
    "DATAFRAMES_NAMES = [CONF_LOG_PREFIX, FLST_LOG_PREFIX, GEO_LOG_PREFIX, LOS_LOG_PREFIX, REG_LOG_PREFIX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give access to the constants that defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(str(Path(Path().absolute().parent, 'platform_code')))\n",
    "from schemas.tables_attributes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataframes(files_names: List[str], loading_path: str, spark: SparkSession) -> Dict[str, DataFrame]:\n",
    "    \"\"\" Loads the dataframes which macht the file names passed by arguments.\n",
    "    The method read from the config the path were to read the files, which\n",
    "    matches the folder where the files are saved in `save_dataframes_dict()`.\n",
    "\n",
    "    :param files_names: list of the names of the files.\n",
    "    :param loading_path: path were the files are saved.\n",
    "    :param spark: spark session.\n",
    "    :return: dictionary with the dataframes loaded from the files, with the\n",
    "     file name as key.\n",
    "    \"\"\"\n",
    "    dataframes = dict()\n",
    "\n",
    "    for file_name in files_names:\n",
    "        file_path = Path(loading_path, f'{file_name.lower()}.parquet')\n",
    "        logger.info('Loading dataframe from `{}`.', file_path)\n",
    "        df = spark.read.parquet(str(file_path))\n",
    "        dataframes[file_name] = df\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Notebook').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 14:21:38.233 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\conflog.parquet`.\n",
      "2022-03-28 14:21:47.192 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\flstlog.parquet`.\n",
      "2022-03-28 14:21:47.460 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\geolog.parquet`.\n",
      "2022-03-28 14:21:47.684 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\loslog.parquet`.\n",
      "2022-03-28 14:21:47.869 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\reglog.parquet`.\n"
     ]
    }
   ],
   "source": [
    "input_dataframes = load_dataframes(DATAFRAMES_NAMES, LOADING_PATH, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dataframes[REG_LOG_PREFIX].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV-2\n",
    "import geopy.distance\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def get_coordinates_distance(origin_latitude: float, origin_longitude: float,\n",
    "                             destination_latitude: float, destination_longitude: float) -> float:\n",
    "    \"\"\" Calculates the distance in meters between two world coordinates.\n",
    "\n",
    "    :param origin_latitude: origin latitude point.\n",
    "    :param origin_longitude: origin longitude point.\n",
    "    :param destination_latitude: destination latitude point.\n",
    "    :param destination_longitude: destination longitude point.\n",
    "    :return: distance in meters.\n",
    "    \"\"\"\n",
    "    origin_tuple = (origin_latitude, origin_longitude)\n",
    "    destination_tuple = (destination_latitude, destination_longitude)\n",
    "    # TODO: direct distance calculation (in meters) between two points, is this approach correct?\n",
    "    return geopy.distance.distance(origin_tuple, destination_tuple).m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = input_dataframes[REG_LOG_PREFIX]\n",
    "\n",
    "# compute next latitude and longitude foreach row \n",
    "window = Window.partitionBy(SCENARIO_NAME).orderBy(SIMULATION_TIME)    \n",
    "dataframe = dataframe.withColumn(\"NEXT_LATITUDE\",lag(LATITUDE, -1).over(window)).withColumn(\"NEXT_LONGITUDE\",lag(LONGITUDE, -1).over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with NEXT_LATITUDE and NEXT_LONGITUDE null (they are the rows of separation between scenarios)\n",
    "dataframe = dataframe.na.drop(subset=[\"NEXT_LATITUDE\",\"NEXT_LONGITUDE\"])\n",
    "#Check it\n",
    "dataframe.filter(col(\"NEXT_LATITUDE\").isNull() | col(\"NEXT_LATITUDE\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumn(\"DIST_NEXT_POINT\", get_coordinates_distance(LATITUDE, LONGITUDE, \"NEXT_LATITUDE\", \"NEXT_LONGITUDE\"))\n",
    "dataframe = dataframe.withColumn(\"WEIGHT_SEGMENT\",  col(ALTITUDE)*col(\"DIST_NEXT_POINT\"))\n",
    "df = dataframe.groupby(SCENARIO_NAME, ACID).agg(F.sum(col(\"WEIGHT_SEGMENT\")).alias(\"FP_ENV2\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average per scenario\n",
    "df = df.groupBy(SCENARIO_NAME).agg(mean(\"FP_ENV2\").alias(ENV2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV4\n",
    "dataframe4 = input_dataframes[REG_LOG_PREFIX]\n",
    "dataframe4 = dataframe4.groupby(SCENARIO_NAME, ACID).agg(F.max(ALTITUDE).alias(\"MAX_ALTITUDE\"), F.min(ALTITUDE).alias(\"MIN_ALTITUDE\"))\n",
    "dataframe4 = dataframe4.withColumn(\"DIFF_ALTITUDE\", col(\"MAX_ALTITUDE\") - col(\"MIN_ALTITUDE\"))\n",
    "avg_delay = dataframe4.select(mean(\"DIFF_ALTITUDE\").alias(\"MEAN_DIFF_ALTITUDE\"))\n",
    "dataframe4 = dataframe4.join(avg_delay, how='outer')\n",
    "dataframe4.show()\n",
    "\n",
    "dataframe4 = dataframe4.withColumn(ENV4, col(\"DIFF_ALTITUDE\")/col(\"MEAN_DIFF_ALTITUDE\"))\n",
    "dataframe4 = dataframe4.select(SCENARIO_NAME, ACID, ENV4)\n",
    "dataframe4.show()\n",
    "# dataframe5 = dataframe4.groupby(SCENARIO_NAME).agg(F.max('MAX_ALTITUDE').alias(\"MAX_ALTITUDE_SCN\"), F.min('MIN_ALTITUDE').alias(\"MIN_ALTITUDE_SCN\"))\n",
    "# dataframe5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_coordinates_distance(origin_latitude: float, origin_longitude: float,\n",
    "                             destination_latitude: float, destination_longitude: float) -> float:\n",
    "    \"\"\" Calculates the distance in meters between two world coordinates.\n",
    "\n",
    "    :param origin_latitude: origin latitude point.\n",
    "    :param origin_longitude: origin longitude point.\n",
    "    :param destination_latitude: destination latitude point.\n",
    "    :param destination_longitude: destination longitude point.\n",
    "    :return: distance in meters.\n",
    "    \"\"\"\n",
    "    origin_tuple = (origin_latitude, origin_longitude)\n",
    "    destination_tuple = (destination_latitude, destination_longitude)\n",
    "    # TODO: direct distance calculation (in meters) between two points, is this approach correct?\n",
    "    return geopy.distance.distance(origin_tuple, destination_tuple).m\n",
    "\n",
    "\n",
    "@udf\n",
    "def in_circle (x_center, y_center, x, y, radius):\n",
    "    return get_coordinates_distance(x_center, y_center, x, y) <= radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import great_circle\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(\"double\")\n",
    "def great_circle_udf(x, y):\n",
    "    return great_circle(x, y).kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- REG_ID: integer (nullable = true)\n",
      " |-- Scenario: string (nullable = true)\n",
      " |-- Simulation_time: double (nullable = true)\n",
      " |-- ACID: string (nullable = true)\n",
      " |-- ALT: double (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      "\n",
      "+------+------------------+---------------+-----+------------------+-----------+-----------+\n",
      "|REG_ID|          Scenario|Simulation_time| ACID|               ALT|        LAT|        LON|\n",
      "+------+------------------+---------------+-----+------------------+-----------+-----------+\n",
      "| 76800|1_very_low_40_8_R2|         2880.0|D2320|             45.72|48.15636078|16.32453111|\n",
      "| 76801|1_very_low_40_8_R2|         2880.0|D2340|128.01600000000002|48.17504097|16.36041054|\n",
      "| 76802|1_very_low_40_8_R2|         2880.0|D2337|            73.152|48.23478639|16.42459103|\n",
      "| 76803|1_very_low_40_8_R2|         2880.0|D2342|  136.185390000024|48.19298318|16.39227946|\n",
      "| 76804|1_very_low_40_8_R2|         2880.0|D2343|          31.83255|48.21440571|16.35687219|\n",
      "| 76805|1_very_low_40_8_R2|         2880.0|D2346|            73.152|48.19509474|16.39687844|\n",
      "| 76806|1_very_low_40_8_R2|         2880.0|D2358|            9.8679|48.20259996|16.30276413|\n",
      "| 76807|1_very_low_40_8_R2|         2880.0|D2364|            36.576|48.17207706|16.33102202|\n",
      "| 76808|1_very_low_40_8_R2|         2880.0|D2368|             9.144| 48.2029329|16.40249793|\n",
      "| 76809|1_very_low_40_8_R2|         2880.0|D2366|27.432000000000002|48.18712748|16.39761369|\n",
      "| 76810|1_very_low_40_8_R2|         2880.0|D2374| 64.00800000000001|48.25741419|16.36644796|\n",
      "| 76811|1_very_low_40_8_R2|         2880.0|D2387|54.864000000000004|48.21769203| 16.3388323|\n",
      "| 76812|1_very_low_40_8_R2|         2880.0|D2390|49.444609999583996|48.21000289|16.35541169|\n",
      "| 76813|1_very_low_40_8_R2|         2880.0|D2392| 64.00800000000001|48.24848122|16.30479889|\n",
      "| 76814|1_very_low_40_8_R2|         2880.0|D2396|   134.84661000144|48.20109701|16.36781158|\n",
      "| 76815|1_very_low_40_8_R2|         2880.0|D2399|            36.576|48.16658901|16.30623433|\n",
      "| 76816|1_very_low_40_8_R2|         2880.0|D2400| 88.27061000148001| 48.1921775|16.39427255|\n",
      "| 76817|1_very_low_40_8_R2|         2880.0|D2405|27.432000000000002|48.20279291|16.34245164|\n",
      "| 76818|1_very_low_40_8_R2|         2880.0|D2421|54.864000000000004|48.18905068|16.35061377|\n",
      "| 76819|1_very_low_40_8_R2|         2880.0|D2422|          82.08645|48.21666121|16.36242682|\n",
      "+------+------------------+---------------+-----+------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+------------------+---------------+----+------------------+-----------+-----------+-------------------+-------------------+\n",
      "|REG_ID|          Scenario|Simulation_time|ACID|               ALT|        LAT|        LON|           distance|        noise_level|\n",
      "+------+------------------+---------------+----+------------------+-----------+-----------+-------------------+-------------------+\n",
      "|   258|1_very_low_40_8_R2|          120.0|  D1|27.222450000000002|48.18967447|16.36144724|  4.606060930198562|-1.3266593572581593|\n",
      "|   259|1_very_low_40_8_R2|          120.0|  D2|27.432000000000002|48.19099305|  16.294872|  4.434718081880119|-1.2937320332845839|\n",
      "|   261|1_very_low_40_8_R2|          120.0|  D4|             9.144|48.16038135|16.32312361|0.45909605662755637| 0.6761928751181322|\n",
      "|   262|1_very_low_40_8_R2|          120.0|  D5|            36.576|48.21710835|16.30520574|  6.905091983622719|-1.6783389364922097|\n",
      "|   263|1_very_low_40_8_R2|          120.0|  D6|             9.144|48.19249925|16.34808551|  4.381591300055195|-1.2832637312012474|\n",
      "|   264|1_very_low_40_8_R2|          120.0|  D7| 64.00800000000001|48.19172915|16.43549346|  9.119722902109068|-1.9199632854477917|\n",
      "|   266|1_very_low_40_8_R2|          120.0|  D9|27.432000000000002|48.19537743|16.40817581|   7.56903049874351|-1.7580805103844557|\n",
      "|   267|1_very_low_40_8_R2|          120.0| D10|           35.7759|48.13801726|16.33173084| 2.1084922162261917|-0.6479440041983231|\n",
      "|   268|1_very_low_40_8_R2|          120.0| D11|             45.72|48.16507652|16.41131346|  6.509365605579786|-1.6270773296951448|\n",
      "|   270|1_very_low_40_8_R2|          120.0| D13| 64.00800000000001|48.16106051|16.38590491| 4.5822890289718865|-1.3221649578904322|\n",
      "|   271|1_very_low_40_8_R2|          120.0| D14|   32.000510000376|48.20641412|16.36043738|  6.169600642158815|-1.5805141061799501|\n",
      "|   272|1_very_low_40_8_R2|          120.0| D15|             9.144|48.21035369|16.42588896|  9.618420595395893|-1.9662075280635898|\n",
      "|   273|1_very_low_40_8_R2|          120.0| D16|            18.288|48.19420697|16.35393531|  4.739598027307904|-1.3514830200887558|\n",
      "|   274|1_very_low_40_8_R2|          120.0| D18|            73.152| 48.1597805|16.37581755|  3.823168715125852|-1.1648469274172935|\n",
      "|   275|1_very_low_40_8_R2|          120.0| D19|27.432000000000002|48.21331814|16.31479294| 6.3743946551685005|-1.6088778965170418|\n",
      "|   277|1_very_low_40_8_R2|          120.0| D21| 64.00800000000001|48.14244864|16.36600417|  3.443818563493258|-1.0740805254300452|\n",
      "|   280|1_very_low_40_8_R2|          120.0| D24|27.432000000000002|48.19021077|16.32129155|  3.771610603525997|-1.1530536960894489|\n",
      "|   281|1_very_low_40_8_R2|          120.0| D25|             9.144|48.18860541|16.35174846|  4.114480378028564|-1.2286299910264666|\n",
      "|   285|1_very_low_40_8_R2|          120.0| D29|   74.677390000776|48.19953311| 16.3281977|  4.808242920087848|-1.3639728015953003|\n",
      "|   286|1_very_low_40_8_R2|          120.0| D30|            18.288|48.18048831|16.42671208|  8.038727554810462|-1.8103746199698756|\n",
      "+------+------------------+---------------+----+------------------+-----------+-----------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ENV3\n",
    "from pyspark.sql.functions import lit, struct\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import pow, col, log\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "x_center = 48.15636078 #lat\n",
    "y_center = 16.32453111 #lon\n",
    "radius_roi = 10 #meters\n",
    "altitude_roi = 100 #meters\n",
    "time_roi = 120 #seconds\n",
    "\n",
    "\n",
    "dataframe3 = input_dataframes[REG_LOG_PREFIX]\n",
    "dataframe3.printSchema() \n",
    "dataframe3.show()\n",
    "\n",
    "point = struct(lit(x_center), lit(y_center))\n",
    "udf_func = udf(great_circle_udf,DoubleType()) #Creating a 'User Defined Function' to calculate distance between two points.\n",
    "dataframe3 = dataframe3.withColumn(\"distance\", udf_func(point, struct(col(LATITUDE), col(LONGITUDE)))).filter((col(\"distance\") <= radius_roi) & (col(ALTITUDE)<= altitude_roi) & (col(SIMULATION_TIME) == time_roi)) #Creating column \"distance\" based on function 'get_distance'\n",
    "#dataframe3 = dataframe3.withColumn(\"noise_level\", 10*log(1/pow(col(\"distance\"),2)))\n",
    "dataframe3 = dataframe3.withColumn(\"noise_level\", log10(1/pow(col(\"distance\"),2)))\n",
    "dataframe3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|          Scenario|   sum(noise_level)|\n",
      "+------------------+-------------------+\n",
      "|1_very_low_40_8_W1|-188.96845944379098|\n",
      "|1_very_low_40_8_R2|-188.96845944379098|\n",
      "|2_very_low_40_8_W1|-188.96845944379098|\n",
      "|3_very_low_40_8_W1|-188.96845944379098|\n",
      "|3_very_low_40_8_R2|-188.96845944379098|\n",
      "|2_very_low_40_8_R2|-188.96845944379098|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe3.groupBy(SCENARIO_NAME).agg(sum(\"noise_level\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
