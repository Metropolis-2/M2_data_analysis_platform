{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "import geopy.distance\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from geopy.distance import great_circle\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit, struct\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import pow, col, log\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CONF_LOG_PREFIX = 'CONFLOG'\n",
    "FLST_LOG_PREFIX = 'FLSTLOG'\n",
    "GEO_LOG_PREFIX = 'GEOLOG'\n",
    "LOS_LOG_PREFIX = 'LOSLOG'\n",
    "REG_LOG_PREFIX = 'REGLOG'\n",
    "LOADING_PATH = '../output'\n",
    "DATAFRAMES_NAMES = [CONF_LOG_PREFIX, FLST_LOG_PREFIX, GEO_LOG_PREFIX, LOS_LOG_PREFIX, REG_LOG_PREFIX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give access to the constants that defines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(str(Path(Path().absolute().parent, 'platform_code')))\n",
    "from schemas.tables_attributes import *\n",
    "from utils.config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataframes(files_names: List[str], loading_path: str, spark: SparkSession) -> Dict[str, DataFrame]:\n",
    "    \"\"\" Loads the dataframes which macht the file names passed by arguments.\n",
    "    The method read from the config the path were to read the files, which\n",
    "    matches the folder where the files are saved in `save_dataframes_dict()`.\n",
    "\n",
    "    :param files_names: list of the names of the files.\n",
    "    :param loading_path: path were the files are saved.\n",
    "    :param spark: spark session.\n",
    "    :return: dictionary with the dataframes loaded from the files, with the\n",
    "     file name as key.\n",
    "    \"\"\"\n",
    "    dataframes = dict()\n",
    "\n",
    "    for file_name in files_names:\n",
    "        file_path = Path(loading_path, f'{file_name.lower()}.parquet')\n",
    "        logger.info('Loading dataframe from `{}`.', file_path)\n",
    "        df = spark.read.parquet(str(file_path))\n",
    "        dataframes[file_name] = df\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Notebook').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 14:19:30.146 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\conflog.parquet`.\n",
      "2022-04-04 14:19:33.075 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\flstlog.parquet`.\n",
      "2022-04-04 14:19:33.193 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\geolog.parquet`.\n",
      "2022-04-04 14:19:33.278 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\loslog.parquet`.\n",
      "2022-04-04 14:19:33.365 | INFO     | __main__:load_dataframes:16 - Loading dataframe from `..\\output\\reglog.parquet`.\n"
     ]
    }
   ],
   "source": [
    "input_dataframes = load_dataframes(DATAFRAMES_NAMES, LOADING_PATH, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def get_coordinates_distance(origin_latitude: float, origin_longitude: float,\n",
    "                             destination_latitude: float, destination_longitude: float) -> float:\n",
    "    \"\"\" Calculates the distance in meters between two world coordinates.\n",
    "\n",
    "    :param origin_latitude: origin latitude point.\n",
    "    :param origin_longitude: origin longitude point.\n",
    "    :param destination_latitude: destination latitude point.\n",
    "    :param destination_longitude: destination longitude point.\n",
    "    :return: distance in meters.\n",
    "    \"\"\"\n",
    "    origin_tuple = (origin_latitude, origin_longitude)\n",
    "    destination_tuple = (destination_latitude, destination_longitude)\n",
    "    # TODO: direct distance calculation (in meters) between two points, is this approach correct?\n",
    "    return geopy.distance.distance(origin_tuple, destination_tuple).m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV-2: Weighted average altitude\n",
    "Average flight level weighed by the length flown at each flight level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = input_dataframes[REG_LOG_PREFIX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check the log for a given drone in a given scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe.where(col(SCENARIO_NAME) == '1_very_low_40_8_R2').where(col(ACID) == 'D1').orderBy(SIMULATION_TIME, ACID).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column with the next coordinates system for the same drone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "window = Window.partitionBy(SCENARIO_NAME, ACID).orderBy(SIMULATION_TIME)\n",
    "next_step = dataframe.withColumn(\"NEXT_LATITUDE\",lag(LATITUDE, -1).over(window)).withColumn(\"NEXT_LONGITUDE\",lag(LONGITUDE, -1).over(window)).withColumn(\"NEXT_ALTITUDE\",lag(ALTITUDE, -1).over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#next_step.where(col(SCENARIO_NAME) == '1_very_low_40_8_R2').where(col(ACID) == 'D1').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with NEXT_LATITUDE and NEXT_LONGITUDE null (they are the rows of separation between scenarios)\n",
    "dataframe = next_step.na.drop(subset=[\"NEXT_LATITUDE\",\"NEXT_LONGITUDE\"])\n",
    "#Check it\n",
    "dataframe.filter(col(\"NEXT_LATITUDE\").isNull() | col(\"NEXT_LATITUDE\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = dataframe.withColumn(\"SEGMENT_LENGTH\",get_coordinates_distance(LATITUDE, LONGITUDE, \"NEXT_LATITUDE\", \"NEXT_LONGITUDE\")).withColumn(\"SEGMENT_ALTITUDE\", (col(ALTITUDE)+col(\"NEXT_ALTITUDE\"))/2)\n",
    "dataframe = dataframe.withColumn(\"SEGMENT_WEIGHT\", col(\"SEGMENT_ALTITUDE\") * col(\"SEGMENT_LENGTH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe.select(REG_ID, SCENARIO_NAME, ACID, \"SEGMENT_LENGTH\", \"SEGMENT_ALTITUDE\", \"SEGMENT_WEIGHT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.groupby(SCENARIO_NAME).agg(sum(col(\"SEGMENT_WEIGHT\")), sum(col(\"SEGMENT_LENGTH\"))).withColumn(ENV2, col(\"sum(SEGMENT_WEIGHT)\")/col(\"sum(SEGMENT_LENGTH)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV4\n",
    "dataframe4 = input_dataframes[REG_LOG_PREFIX]\n",
    "dataframe4 = dataframe4.groupby(SCENARIO_NAME, ACID).agg(F.max(ALTITUDE).alias(\"MAX_ALTITUDE\"), F.min(ALTITUDE).alias(\"MIN_ALTITUDE\"))\n",
    "dataframe4 = dataframe4.withColumn(\"DIFF_ALTITUDE\", col(\"MAX_ALTITUDE\") - col(\"MIN_ALTITUDE\"))\n",
    "avg_delay = dataframe4.select(mean(\"DIFF_ALTITUDE\").alias(\"MEAN_DIFF_ALTITUDE\"))\n",
    "dataframe4 = dataframe4.join(avg_delay, how='outer')\n",
    "dataframe4.show()\n",
    "\n",
    "dataframe4 = dataframe4.withColumn(ENV4, col(\"DIFF_ALTITUDE\")/col(\"MEAN_DIFF_ALTITUDE\"))\n",
    "dataframe4 = dataframe4.select(SCENARIO_NAME, ACID, ENV4)\n",
    "dataframe4.show()\n",
    "# dataframe5 = dataframe4.groupby(SCENARIO_NAME).agg(F.max('MAX_ALTITUDE').alias(\"MAX_ALTITUDE_SCN\"), F.min('MIN_ALTITUDE').alias(\"MIN_ALTITUDE_SCN\"))\n",
    "# dataframe5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_coordinates_distance(origin_latitude: float, origin_longitude: float,\n",
    "                             destination_latitude: float, destination_longitude: float) -> float:\n",
    "    \"\"\" Calculates the distance in meters between two world coordinates.\n",
    "\n",
    "    :param origin_latitude: origin latitude point.\n",
    "    :param origin_longitude: origin longitude point.\n",
    "    :param destination_latitude: destination latitude point.\n",
    "    :param destination_longitude: destination longitude point.\n",
    "    :return: distance in meters.\n",
    "    \"\"\"\n",
    "    origin_tuple = (origin_latitude, origin_longitude)\n",
    "    destination_tuple = (destination_latitude, destination_longitude)\n",
    "    # TODO: direct distance calculation (in meters) between two points, is this approach correct?\n",
    "    return geopy.distance.distance(origin_tuple, destination_tuple).m\n",
    "\n",
    "\n",
    "@udf\n",
    "def in_circle (x_center, y_center, x, y, radius):\n",
    "    return get_coordinates_distance(x_center, y_center, x, y) <= radius\n",
    "\n",
    "@udf(\"double\")\n",
    "def great_circle_udf(x, y):\n",
    "    return great_circle(x, y).kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(settings.x_center)\n",
    "print(settings.time_roi)\n",
    "print(settings.radius_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEnv3points(geojson):\n",
    "    return gpd.read_file(geojson)[0:10] #only first 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"env3_points.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in df_points.iterrows():\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV3\n",
    "dataframe3 = input_dataframes[REG_LOG_PREFIX]\n",
    "udf_func = udf(great_circle_udf,DoubleType()) #Creating a 'User Defined Function' to calculate distance between two points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'LAT' given input columns: [ENV3_p0, Scenario];\n'Project [Scenario#117, ENV3_p0#2840, great_circle_udf(struct(col1, 48.216866, col2, 16.36884), struct(LAT, 'LAT, LON, 'LON)) AS distance#2844]\n+- Aggregate [Scenario#117], [Scenario#117, sum(sound_intensity#2820) AS ENV3_p0#2840]\n   +- Project [REG_ID#116, Scenario#117, Simulation_time#118, ACID#119, ALT#120, LAT#121, LON#122, distance#2811, (cast(1 as double) / cast(POWER((distance#2811 / 9.144), 2.0) as double)) AS sound_intensity#2820]\n      +- Filter (distance#2811 <= cast(16 as double))\n         +- Project [REG_ID#116, Scenario#117, Simulation_time#118, ACID#119, ALT#120, LAT#121, LON#122, great_circle_udf(struct(col1, 48.217423, col2, 16.371279), struct(LAT, LAT#121, LON, LON#122)) AS distance#2811]\n            +- Filter (Simulation_time#118 = cast(120 as double))\n               +- Relation [REG_ID#116,Scenario#117,Simulation_time#118,ACID#119,ALT#120,LAT#121,LON#122] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df_points\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mx, df_points\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39my)):\n\u001b[0;32m     10\u001b[0m     point \u001b[38;5;241m=\u001b[39m struct(lit(y), lit(x))\n\u001b[1;32m---> 12\u001b[0m     aux_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43maux_dataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreat_circle_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLATITUDE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLONGITUDE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     aux_dataframe \u001b[38;5;241m=\u001b[39m aux_dataframe\u001b[38;5;241m.\u001b[39mfilter((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m))\n\u001b[0;32m     14\u001b[0m     aux_dataframe \u001b[38;5;241m=\u001b[39m aux_dataframe\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound_intensity\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mpow\u001b[39m((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m/\u001b[39msettings\u001b[38;5;241m.\u001b[39mflight_altitude\u001b[38;5;241m.\u001b[39mlowest), \u001b[38;5;241m2\u001b[39m)))\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\m2_pyspark\\lib\\site-packages\\pyspark\\sql\\dataframe.py:2478\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\m2_pyspark\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\Miniconda3\\envs\\m2_pyspark\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'LAT' given input columns: [ENV3_p0, Scenario];\n'Project [Scenario#117, ENV3_p0#2840, great_circle_udf(struct(col1, 48.216866, col2, 16.36884), struct(LAT, 'LAT, LON, 'LON)) AS distance#2844]\n+- Aggregate [Scenario#117], [Scenario#117, sum(sound_intensity#2820) AS ENV3_p0#2840]\n   +- Project [REG_ID#116, Scenario#117, Simulation_time#118, ACID#119, ALT#120, LAT#121, LON#122, distance#2811, (cast(1 as double) / cast(POWER((distance#2811 / 9.144), 2.0) as double)) AS sound_intensity#2820]\n      +- Filter (distance#2811 <= cast(16 as double))\n         +- Project [REG_ID#116, Scenario#117, Simulation_time#118, ACID#119, ALT#120, LAT#121, LON#122, great_circle_udf(struct(col1, 48.217423, col2, 16.371279), struct(LAT, LAT#121, LON, LON#122)) AS distance#2811]\n            +- Filter (Simulation_time#118 = cast(120 as double))\n               +- Relation [REG_ID#116,Scenario#117,Simulation_time#118,ACID#119,ALT#120,LAT#121,LON#122] parquet\n"
     ]
    }
   ],
   "source": [
    "#ENV3\n",
    "dataframe3 = input_dataframes[REG_LOG_PREFIX]\n",
    "\n",
    "#dataframe3.printSchema() \n",
    "#dataframe3.show()\n",
    "df_points = loadEnv3points(settings.geojson.path)\n",
    "\n",
    "for i,(x, y) in enumerate(zip(df_points.geometry.x, df_points.geometry.y)):\n",
    "    point = struct(lit(y), lit(x))\n",
    "    \n",
    "    aux_dataframe = dataframe3.filter(col(SIMULATION_TIME) == settings.env3.time_roi)\n",
    "    aux_dataframe = aux_dataframe.withColumn(\"distance\", great_circle_udf(point, struct(col(LATITUDE), col(LONGITUDE))))\n",
    "    aux_dataframe = aux_dataframe.filter((col(\"distance\") <= 16))\n",
    "    aux_dataframe = aux_dataframe.withColumn(\"sound_intensity\", 1/(pow((col(\"distance\")/settings.flight_altitude.lowest), 2)))\n",
    "    aux_dataframe = aux_dataframe.groupby(SCENARIO_NAME).agg(sum(\"sound_intensity\").alias(f\"ENV3_p{i}\"))\n",
    "    \n",
    "    \n",
    "    if(i==0):\n",
    "        final_dataframe = aux_dataframe\n",
    "    else:\n",
    "        final_dataframe = final_dataframe.join(aux_dataframe, SCENARIO_NAME)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+-----------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|          Scenario|          ENV3_p0|          ENV3_p1|          ENV3_p2|           ENV3_p3|          ENV3_p4|           ENV3_p5|          ENV3_p6|           ENV3_p7|          ENV3_p8|          ENV3_p9|\n",
      "+------------------+-----------------+-----------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "|1_very_low_40_8_W1|5330.543421714847|10369.50705236441|6099.637040471681|14712.051075586354|6571.919849418305|6860.9610221713965|64300.00334081777|15007.990403937401|5502.500571574826|7188.049529260242|\n",
      "|1_very_low_40_8_R2|5330.543421714847|10369.50705236441|6099.637040471681|14712.051075586354|6571.919849418305|6860.9610221713965|64300.00334081777|15007.990403937401|5502.500571574826|7188.049529260242|\n",
      "|2_very_low_40_8_W1|5330.543421714847|10369.50705236441|6099.637040471681|14712.051075586354|6571.919849418305|6860.9610221713965|64300.00334081777|15007.990403937401|5502.500571574826|7188.049529260242|\n",
      "|3_very_low_40_8_W1|5330.543421714847|10369.50705236441|6099.637040471681|14712.051075586354|6571.919849418305|6860.9610221713965|64300.00334081777|15007.990403937401|5502.500571574826|7188.049529260242|\n",
      "|3_very_low_40_8_R2|5330.543421714847|10369.50705236441|6099.637040471681|14712.051075586354|6571.919849418305|6860.9610221713965|64300.00334081777|15007.990403937401|5502.500571574826|7188.049529260242|\n",
      "|2_very_low_40_8_R2|5330.543421714847|10369.50705236441|6099.637040471681|14712.051075586354|6571.919849418305|6860.9610221713965|64300.00334081777|15007.990403937401|5502.500571574826|7188.049529260242|\n",
      "+------------------+-----------------+-----------------+-----------------+------------------+-----------------+------------------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_dataframe.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "point = struct(lit(x_center), lit(y_center))\n",
    "udf_func = udf(great_circle_udf,DoubleType()) #Creating a 'User Defined Function' to calculate distance between two points.\n",
    "dataframe3 = dataframe3.withColumn(\"distance\", udf_func(point, struct(col(LATITUDE), col(LONGITUDE)))).filter((col(\"distance\") <= radius_roi) & (col(ALTITUDE)<= altitude_roi) & (col(SIMULATION_TIME) == time_roi)) #Creating column \"distance\" based on function 'get_distance'\n",
    "#dataframe3 = dataframe3.withColumn(\"noise_level\", 10*log(1/pow(col(\"distance\"),2)))\n",
    "dataframe3 = dataframe3.withColumn(\"noise_level\", log10(1/pow(col(\"distance\"),2)))\n",
    "dataframe3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe3.groupBy(SCENARIO_NAME).agg(sum(\"noise_level\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataframe = input_dataframes[REG_LOG_PREFIX]\n",
    "\n",
    "point = struct(lit(settings.x_center), lit(settings.y_center))\n",
    "\n",
    "# TODO: ? Define formula for the sound depending on the distance to the point.\n",
    "# TODO: ? How many points and how to define.\n",
    "return dataframe.filter(col(SIMULATION_TIME) == settings.time_roi) \\\n",
    "    .withColumn(\"distance\", great_circle_udf(point, struct(col(LATITUDE), col(LONGITUDE)))) \\\n",
    "    .filter((col(\"distance\") <= settings.radius_roi) & (col(ALTITUDE) <= settings.altitude_roi)) \\\n",
    "    .withColumn(\"noise_level\", log10(1 / pow(col(\"distance\"), 2))) \\\n",
    "    .groupBy(SCENARIO_NAME).agg(sum(\"noise_level\").alias(ENV3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
