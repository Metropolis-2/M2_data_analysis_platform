{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "The next cell adds the platform code in order to access the parsing methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "sys.path.append(str(Path(Path().absolute().parent, 'platform_code')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can import as if we were working inside the platform code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from parse.flst_log_parser import FLST_LOG_TRANSFORMATIONS\n",
    "from schemas.flst_log_schema import FLST_LOG_FILE_SCHEMA\n",
    "from parse.parser_constants import FLST_LOG_PREFIX\n",
    "from schemas.geo_log_schema import GEO_LOG_FILE_SCHEMA\n",
    "from parse.geo_log_parser import GEO_LOG_TRANSFORMATIONS\n",
    "from schemas.reg_log_schema import REG_LOG_SCHEMA\n",
    "from parse.reg_log_parser import REG_LOG_TRANSFORMATIONS\n",
    "from schemas.los_log_schema import LOS_LOG_FILE_SCHEMA\n",
    "from parse.los_log_parser import LOS_LOG_TRANSFORMATIONS\n",
    "from schemas.conf_log_schema import CONF_LOG_FILE_SCHEMA\n",
    "from parse.conf_log_parser import CONF_LOG_TRANSFORMATIONS\n",
    "from parse.parser_constants import CONF_LOG_PREFIX, LOS_LOG_PREFIX, GEO_LOG_PREFIX, REG_LOG_PREFIX\n",
    "from parse.generic_parser import parse_flight_intentions, parse_log_files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt/shared/repos/metropolis/M2_data_analysis_platform/notebooks/saf_metrics.ipynb'\n",
    "# Configuration for the log names with the schema associated and the transformations\n",
    "# required after the read of the log file.\n",
    "# Prefix: (schema, transformations)\n",
    "PARSE_CONFIG = {\n",
    "    CONF_LOG_PREFIX: (CONF_LOG_FILE_SCHEMA, CONF_LOG_TRANSFORMATIONS),\n",
    "    LOS_LOG_PREFIX: (LOS_LOG_FILE_SCHEMA, LOS_LOG_TRANSFORMATIONS),\n",
    "    GEO_LOG_PREFIX: (GEO_LOG_FILE_SCHEMA, GEO_LOG_TRANSFORMATIONS),\n",
    "    FLST_LOG_PREFIX: (FLST_LOG_FILE_SCHEMA, FLST_LOG_TRANSFORMATIONS),\n",
    "    REG_LOG_PREFIX: (REG_LOG_SCHEMA, REG_LOG_TRANSFORMATIONS)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/23 13:02:42 WARN Utils: Your hostname, GRILUX-DEV resolves to a loopback address: 127.0.1.1; using 192.168.2.150 instead (on interface wlp3s0)\n",
      "22/03/23 13:02:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/23 13:02:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(DATA_PATH).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 13:02:56.677 | DEBUG    | parse.parser_utils:get_scenario_data_from_fp_int:56 - Scenario data string obtained: very_low_40_8_R2.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adrigrillo/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/home/adrigrillo/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/adrigrillo/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/serializers.py:437\u001B[0m, in \u001B[0;36mCloudPickleSerializer.dumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcloudpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mPickleError:\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001B[0m, in \u001B[0;36mdumps\u001B[0;34m(obj, protocol, buffer_callback)\u001B[0m\n\u001B[1;32m     70\u001B[0m cp \u001B[38;5;241m=\u001B[39m CloudPickler(\n\u001B[1;32m     71\u001B[0m     file, protocol\u001B[38;5;241m=\u001B[39mprotocol, buffer_callback\u001B[38;5;241m=\u001B[39mbuffer_callback\n\u001B[1;32m     72\u001B[0m )\n\u001B[0;32m---> 73\u001B[0m \u001B[43mcp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m file\u001B[38;5;241m.\u001B[39mgetvalue()\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:563\u001B[0m, in \u001B[0;36mCloudPickler.dump\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot pickle '_thread.lock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mPicklingError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m fp_intentions_dfs \u001B[38;5;241m=\u001B[39m \u001B[43mparse_flight_intentions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m input_dataframes \u001B[38;5;241m=\u001B[39m parse_log_files(PARSE_CONFIG, fp_intentions_dfs, spark)\n",
      "File \u001B[0;32m/mnt/shared/repos/metropolis/M2_data_analysis_platform/platform_code/parse/generic_parser.py:53\u001B[0m, in \u001B[0;36mparse_flight_intentions\u001B[0;34m(spark)\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m transformation \u001B[38;5;129;01min\u001B[39;00m FP_INT_TRANSFORMATIONS:\n\u001B[1;32m     52\u001B[0m         logger\u001B[38;5;241m.\u001B[39mtrace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mApplying data transformation: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, transformation)\n\u001B[0;32m---> 53\u001B[0m         dataframe \u001B[38;5;241m=\u001B[39m \u001B[43mtransformation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataframe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     fp_int_dataframes[scenario_data] \u001B[38;5;241m=\u001B[39m dataframe\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fp_int_dataframes\n",
      "File \u001B[0;32m/mnt/shared/repos/metropolis/M2_data_analysis_platform/platform_code/parse/fp_int_parser.py:75\u001B[0m, in \u001B[0;36mcalculate_cruising_speed\u001B[0;34m(dataframe)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculate_cruising_speed\u001B[39m(dataframe: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;124;03m\"\"\" Adds the cruising to the dataframe depending on the drone type.\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;03m    :param dataframe: dataframe with the flight plan intention data read from the file.\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03m    :return: dataframe with the column cruising speed added.\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dataframe\u001B[38;5;241m.\u001B[39mwithColumn(CRUISING_SPEED, \u001B[43mget_drone_speed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m(\u001B[49m\u001B[43mVEHICLE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/sql/udf.py:199\u001B[0m, in \u001B[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001B[0;34m(*args)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc, assigned\u001B[38;5;241m=\u001B[39massignments)\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m--> 199\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/sql/udf.py:177\u001B[0m, in \u001B[0;36mUserDefinedFunction.__call__\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols):\n\u001B[0;32m--> 177\u001B[0m     judf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_judf\u001B[49m\n\u001B[1;32m    178\u001B[0m     sc \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Column(judf\u001B[38;5;241m.\u001B[39mapply(_to_seq(sc, cols, _to_java_column)))\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/sql/udf.py:161\u001B[0m, in \u001B[0;36mUserDefinedFunction._judf\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_judf\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001B[39;00m\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001B[39;00m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001B[39;00m\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;66;03m# and should have a minimal performance impact.\u001B[39;00m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_judf_placeholder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 161\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_judf_placeholder \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_judf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_judf_placeholder\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/sql/udf.py:170\u001B[0m, in \u001B[0;36mUserDefinedFunction._create_judf\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    167\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m    168\u001B[0m sc \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msparkContext\n\u001B[0;32m--> 170\u001B[0m wrapped_func \u001B[38;5;241m=\u001B[39m \u001B[43m_wrap_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturnType\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m jdt \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mparseDataType(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturnType\u001B[38;5;241m.\u001B[39mjson())\n\u001B[1;32m    172\u001B[0m judf \u001B[38;5;241m=\u001B[39m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mexecution\u001B[38;5;241m.\u001B[39mpython\u001B[38;5;241m.\u001B[39mUserDefinedPythonFunction(\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name, wrapped_func, jdt, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevalType, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeterministic)\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/sql/udf.py:34\u001B[0m, in \u001B[0;36m_wrap_function\u001B[0;34m(sc, func, returnType)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_function\u001B[39m(sc, func, returnType):\n\u001B[1;32m     33\u001B[0m     command \u001B[38;5;241m=\u001B[39m (func, returnType)\n\u001B[0;32m---> 34\u001B[0m     pickled_command, broadcast_vars, env, includes \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_for_python_RDD\u001B[49m\u001B[43m(\u001B[49m\u001B[43msc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonFunction(\u001B[38;5;28mbytearray\u001B[39m(pickled_command), env, includes, sc\u001B[38;5;241m.\u001B[39mpythonExec,\n\u001B[1;32m     36\u001B[0m                                   sc\u001B[38;5;241m.\u001B[39mpythonVer, broadcast_vars, sc\u001B[38;5;241m.\u001B[39m_javaAccumulator)\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/rdd.py:2816\u001B[0m, in \u001B[0;36m_prepare_for_python_RDD\u001B[0;34m(sc, command)\u001B[0m\n\u001B[1;32m   2813\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_prepare_for_python_RDD\u001B[39m(sc, command):\n\u001B[1;32m   2814\u001B[0m     \u001B[38;5;66;03m# the serialized command will be compressed by broadcast\u001B[39;00m\n\u001B[1;32m   2815\u001B[0m     ser \u001B[38;5;241m=\u001B[39m CloudPickleSerializer()\n\u001B[0;32m-> 2816\u001B[0m     pickled_command \u001B[38;5;241m=\u001B[39m \u001B[43mser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2817\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pickled_command) \u001B[38;5;241m>\u001B[39m sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonUtils\u001B[38;5;241m.\u001B[39mgetBroadcastThreshold(sc\u001B[38;5;241m.\u001B[39m_jsc):  \u001B[38;5;66;03m# Default 1M\u001B[39;00m\n\u001B[1;32m   2818\u001B[0m         \u001B[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001B[39;00m\n\u001B[1;32m   2819\u001B[0m         broadcast \u001B[38;5;241m=\u001B[39m sc\u001B[38;5;241m.\u001B[39mbroadcast(pickled_command)\n",
      "File \u001B[0;32m~/.local/bin/conda/envs/m2n/lib/python3.8/site-packages/pyspark/serializers.py:447\u001B[0m, in \u001B[0;36mCloudPickleSerializer.dumps\u001B[0;34m(self, obj)\u001B[0m\n\u001B[1;32m    445\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not serialize object: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (e\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, emsg)\n\u001B[1;32m    446\u001B[0m print_exec(sys\u001B[38;5;241m.\u001B[39mstderr)\n\u001B[0;32m--> 447\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mPicklingError(msg)\n",
      "\u001B[0;31mPicklingError\u001B[0m: Could not serialize object: TypeError: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "fp_intentions_dfs = parse_flight_intentions(spark)\n",
    "input_dataframes = parse_log_files(PARSE_CONFIG, fp_intentions_dfs, spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}